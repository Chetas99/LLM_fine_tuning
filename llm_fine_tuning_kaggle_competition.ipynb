{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 86518,
          "databundleVersionId": 9809560,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 LLM Classification Fine-tuning (RoBERTa Transformer)\n",
        "\n",
        "This notebook demonstrates fine-tuning of a RoBERTa-based transformer model to classify preferred responses between pairs of chatbot-generated interactions. It's part of the [LLM Classification Fine-tuning Kaggle competition](https://www.kaggle.com/competitions/llm-classification-finetuning).\n",
        "\n",
        "**Author**: Chetas Srinivas  \n",
        "**Date**: March 2025\n"
      ],
      "metadata": {
        "id": "lG6kt9tmZyti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Setup & Installation\n",
        "\n",
        "Install all necessary dependencies including Hugging Face Transformers, datasets, WandB, and other key libraries.\n"
      ],
      "metadata": {
        "id": "adjBXiwhZytk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "import torch\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "# Initialize wandb (it will prompt for API key)\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "iwscihwsdZjm",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📂 Data Loading and Exploration\n",
        "\n",
        "Loading and exploring datasets provided by the competition, which include training and testing data.\n"
      ],
      "metadata": {
        "id": "4W-0cFsOZytm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "zfw3qTdyZytm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🛠️ Data Preprocessing\n",
        "\n",
        "Prepare textual data for classification tasks by combining prompt and responses. Convert multi-class labels to a single integer class for classification.\n"
      ],
      "metadata": {
        "id": "Ss41eI66Zytn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data function\n",
        "def prepare_data(df):\n",
        "    texts = df['prompt'] + ' [SEP] ' + df['response_a'] + ' [SEP] ' + df['response_b']\n",
        "    labels = df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)\n",
        "    return texts.tolist(), labels\n",
        "\n",
        "train_texts, train_labels = prepare_data(train_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FDHXm0SvZytn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Train-Validation Split\n",
        "\n",
        "Splitting the data into training and validation sets to evaluate model performance and avoid overfitting.\n"
      ],
      "metadata": {
        "id": "ejkPuURVZytn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-validation split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.1, random_state=42\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "GDEig86lZytn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🤖 Model and Tokenizer Setup\n",
        "\n",
        "Using a pre-trained RoBERTa model and tokenizer from Hugging Face for sequence classification.\n"
      ],
      "metadata": {
        "id": "jvduRxuFZyto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=3)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bibulKxMZyto"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Tokenization\n",
        "\n",
        "Convert textual data into tokenized sequences suitable for transformer input.\n"
      ],
      "metadata": {
        "id": "z7W1ijcTZytp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data with optimized token length\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch, padding=True, truncation=True, max_length=256)\n",
        "\n",
        "train_encodings = tokenize(train_texts)\n",
        "val_encodings = tokenize(val_texts)"
      ],
      "metadata": {
        "trusted": true,
        "id": "As41gDr0Zytp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📦 PyTorch Dataset Class\n",
        "\n",
        "Custom dataset class to handle tokenized data efficiently during training and evaluation.\n"
      ],
      "metadata": {
        "id": "CtPytpTdZytp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = Dataset(train_encodings, train_labels)\n",
        "val_dataset = Dataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FBicZXZNZytp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀 Model Training\n",
        "\n",
        "Training the RoBERTa model with Hugging Face's Trainer API. The training process is logged and visualized using **Weights & Biases (wandb)**.\n"
      ],
      "metadata": {
        "id": "lhtE1zODZytp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments (wandb integration)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    report_to=\"wandb\",  # Enables wandb logging\n",
        "    run_name=\"llm_classification_finetune\",  # Name your wandb run clearly\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "6Q7OXT6UZytp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Model Evaluation and Predictions\n",
        "\n",
        "Evaluating the trained model on test data to generate predictions for the competition submission.\n"
      ],
      "metadata": {
        "id": "fS5GxqKUZytq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {'accuracy': (preds == p.label_ids).mean()}\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "sOAFOTRSZytq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **⚠️ Note:**  \n",
        "> Training was initially executed on Google Colab, and outputs have been preserved here for quick reference and demonstration purposes.\n"
      ],
      "metadata": {
        "id": "kI0SvaUgZytq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "fiNBLk5XZytq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test data for predictions\n",
        "test_texts = test_df['prompt'] + ' [SEP] ' + test_df['response_a'] + ' [SEP] ' + test_df['response_b']\n",
        "test_encodings = tokenize(test_texts.tolist())\n",
        "test_dataset = Dataset(test_encodings, [0]*len(test_df))\n",
        "\n",
        "# Predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZXcHnHbVZytq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Submission File\n",
        "\n",
        "Generate the final submission file (`submission.csv`) in the format required by Kaggle.\n"
      ],
      "metadata": {
        "id": "IEf0O4ECZytr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submission\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'winner_model_a': (preds == 0).astype(int),\n",
        "    'winner_model_b': (preds == 1).astype(int),\n",
        "    'winner_tie': (preds == 2).astype(int)\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "oCbsg3nlZytr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎯 Conclusion & Future Improvements\n",
        "\n",
        "**Key insights from the current run**:\n",
        "- Initial accuracy was ~37%. Indicates room for improvement through more extensive hyperparameter tuning and longer training.\n",
        "\n",
        "**Possible future improvements**:\n",
        "- Use more advanced models (e.g., DeBERTa).\n",
        "- Increase epochs and fine-tune hyperparameters.\n",
        "- Implement hyperparameter optimization techniques.\n",
        "\n",
        "**Explore further**:\n",
        "- [wandb Dashboard](https://wandb.ai/chetas0131-sacramento-state/huggingface/runs/83onnn68?nw=nwuserchetas0131)\n",
        "- [GitHub Repository](https://github.com/Chetas99/LLM_fine_tuning/tree/main)\n"
      ],
      "metadata": {
        "id": "xPyPWC1tZytr"
      }
    }
  ]
}